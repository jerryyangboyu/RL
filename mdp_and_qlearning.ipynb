{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size='5'> <center> <u>**Reinforcement Learning Study Note**</u></center></font>\n",
    "\n",
    "This notebook is my note on chapter 17 of the book *Dive Into Deep Learning*. First we will look at markov decision process (MDP) assumption. Then we will introduce value iteration. Finally we will implement the Q-learning algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Markov Decision Process (MDP)**\n",
    "\n",
    "It involves 4 basic components:\n",
    "- $\\mathcal{S}$ be the set of states\n",
    "- $\\mathcal{A}$ be the set of actions that an agent can tak at each state\n",
    "- $T: \\mathcal{S} \\times \\mathcal{A} \\times \\mathcal{S} \\rightarrow [0, 1]$ be a transition function which approximiates the probability of transitioning from one state to another given an action. The probability of transitioning from state $s$ to state $s'$ under action $a$ is denoted as $T(s, a, s') = P(s' | s, a)$. Note that the transition function is a probability distribution, therefore $\\sum_{s' \\in \\mathcal{S}} T(s, a, s') = 1$ for all $s \\in \\mathcal{S}$ and $a \\in \\mathcal{A}$.\n",
    "- $r: \\mathcal{S} \\times \\mathcal{A} \\rightarrow \\mathbb{R}$ be a reward function which approximiates the immediate reward received by the agent after taking an action at a state. The reward of an action $a$ take at state $s$ is denoted as $r(s, a)$.\n",
    "\n",
    "These forms the basis of MDP:\n",
    "\n",
    "$$\n",
    "\\textit{MDP} : (\\mathcal{S}, \\mathcal{A}, T, r)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discount Factor**\n",
    "\n",
    "The agent starts with state $s_0 \\in \\mathcal{S}$ and continues to take actions to form a trajectory:\n",
    "\n",
    "$$\n",
    "\\tau = (s_0, a_0, r_0, s_1, a_1, r_1, \\ldots)\n",
    "$$\n",
    "\n",
    "The agent's goal is to maximize the expected sum of rewards, by choosing the best action at each state. The expected sum of rewards is given by:\n",
    "\n",
    "$$\n",
    "R(\\tau) = r_0 + r_1 + \\ldots = \\sum_{t=0}^{\\infty} r_t\n",
    "$$\n",
    "\n",
    "To prevent the case the agent choose to take infinite actions to maximize the expected sum of rewards, we introduce a discount factor $\\gamma \\in [0, 1]$. The expected sum of rewards is then:\n",
    "\n",
    "$$\n",
    "R(\\tau) = r_0 + \\gamma r_1 + \\gamma^2 r_2 + \\ldots = \\sum_{t=0}^{\\infty} \\gamma^t r_t\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Limitation and Other Models**\n",
    "\n",
    "Note that markov system only look at current state to determine the action for next state, and does not consider the history of states. This is a limitation of MDP. However, it is a good approximation for many real-world problems. Some problems can be model otherwise like this:\n",
    "\n",
    "$$\n",
    "s_(t+1) = f(s_t, a_t, s_{t-1}, s_{t-2}, \\ldots)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Stochastic Policy**\n",
    "\n",
    "A policy defines the behavior of the agent at a given state. Usually it is implemented stochastically by specifying probability for each action, denoted as $\\pi(a | s)$. It is a probability distribution over actions given a state. Since it is a probability distribution, we should have $\\sum_{a \\in \\mathcal{A}} \\pi(a | s) = 1$ for all $s \\in \\mathcal{S}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Value Function**\n",
    "\n",
    "A reward is the gain of an agent at the immediate step. The value function determine the total amount of rewards the agent can expect to accumulate in the future. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2l",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
